{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from pyltp import Postagger\n",
    "from pyltp import Segmentor\n",
    "from pyltp import NamedEntityRecognizer\n",
    "\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary_Model:\n",
    "    def __init__(self):\n",
    "        self.LTP_DATA_DIR = r'D:\\ltp\\ltp_data_v3.4.0'\n",
    "#         self.cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "        self.pos_model_path = os.path.join(self.LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "        self.ner_model_path = os.path.join(self.LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`\n",
    "        self.embeddings_index_filename = r'D:\\Assignment\\datasource-master\\sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'\n",
    "        self.stopwords_filename = \"D:\\Assignment\\datasource-master\\chinese_stopwords.txt\"\n",
    "        self.lda_model_filename = 'D:\\\\Assignment\\\\Project_03\\\\lda_news_model'\n",
    "        self.lda_num = 100\n",
    "        self.embedding_index = {}\n",
    "        self.frequence = {}\n",
    "        self.alpah = 1e-4\n",
    "        self.knn_window = 1\n",
    "        self.keywords_window = 3\n",
    "        self.keywords_num = 5\n",
    "        self.long_constraint=200\n",
    "        self.title_weight = 0.5\n",
    "        \n",
    "    ################################################Process Data####################################################\n",
    "    def token(self, string):\n",
    "        return ''.join(re.findall(r'[\\d|\\w]+', string))\n",
    "    \n",
    "    def cut_sentence(self, para):\n",
    "        para = re.sub('([；。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "        para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "        para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "        para = re.sub('([；。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    #     para = re.sub('([，,])', r'\\1\\n', para)\n",
    "        # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "        para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "        # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "        return para.split(\"\\n\")\n",
    "    \n",
    "    def get_sentence_content_with_punctuation(self, news):\n",
    "        news = re.sub(r'\\n', '', news)\n",
    "        if news != '' and news != ' ':\n",
    "            result = self.cut_sentence(news)\n",
    "        return result\n",
    "\n",
    "    def get_sentence_content(self, news_sentence_with_punctuation):\n",
    "        result = []\n",
    "\n",
    "        for sentence in news_sentence_with_punctuation:\n",
    "            temp = self.token(sentence)\n",
    "            result.append(temp)\n",
    "        return result\n",
    "\n",
    "    def get_word_content(self, news_sentence):\n",
    "        result = []\n",
    "    #     segmentor = Segmentor()  # 初始化实例\n",
    "    #     segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "        for sentence in news_sentence:\n",
    "            temp_words = list(jieba.cut(sentence))\n",
    "    #         temp_words = list(segmentor.segment(sentence))  # 使用哈工大pyltp分词进行切词\n",
    "\n",
    "            result.append(temp_words)\n",
    "\n",
    "    #     segmentor.release()  # 释放模型\n",
    "        return result\n",
    "\n",
    "    def get_stopwords(self):\n",
    "        stopwords_dic = open(self.stopwords_filename, encoding= 'utf-8')\n",
    "        stopwords = stopwords_dic.readlines()\n",
    "        stopwords = [w.strip() for w in stopwords]\n",
    "        stopwords_dic.close()\n",
    "        return stopwords\n",
    "    \n",
    "    def del_stopwords(self, word_content):\n",
    "        stop_words = self.get_stopwords()\n",
    "        result = []\n",
    "        for sentence in word_content:\n",
    "            tmp = []\n",
    "            for word in sentence:\n",
    "                if word not in stop_words:\n",
    "                    tmp.append(word)\n",
    "            result.append(tmp)\n",
    "        return result\n",
    "    \n",
    "    def get_name_entity(self, sentence_words_content):\n",
    "        result = []\n",
    "\n",
    "        postagger = Postagger()  # 初始化实例\n",
    "        postagger.load(self.pos_model_path)  # 加载模型\n",
    "\n",
    "        recognizer = NamedEntityRecognizer()  # 初始化实例\n",
    "        recognizer.load(self.ner_model_path)  # 加载模型\n",
    "\n",
    "        for strs in sentence_words_content:\n",
    "            sentence = ''.join(strs)\n",
    "\n",
    "            # words = list(jieba.cut(sentence))  \n",
    "            words = list(jieba.cut(sentence))    # 结巴分词\n",
    "            postags = list(postagger.postag(words))  # 词性标注\n",
    "            netags = list(recognizer.recognize(words, postags))  # 命名实体识别\n",
    "            # tmp=[str(k+1)+'-'+v for k,v in enumerate(netags)]\n",
    "            # print('\\t'.join(tmp))\n",
    "            result.append(netags)\n",
    "    #         print('\\t'.join(netags))\n",
    "    #         print('\\t'.join(words))\n",
    "        postagger.release()  # 释放模型\n",
    "        recognizer.release()  # 释放模型\n",
    "        return result\n",
    "    \n",
    "    def data_process(self, text):\n",
    "        test_with_punctuation = self.get_sentence_content_with_punctuation(text)\n",
    "\n",
    "        test_no_punctuation = self.get_sentence_content(test_with_punctuation)\n",
    "        test_word_with_stopwords = self.get_word_content(test_no_punctuation)\n",
    "        test_word = self.del_stopwords(test_word_with_stopwords)\n",
    "\n",
    "        test_word_name_entity = self.get_name_entity(test_word)\n",
    "\n",
    "        return test_word, test_with_punctuation, test_word_name_entity\n",
    "    \n",
    "    ################################################Process Data####################################################\n",
    "    \n",
    "    \n",
    "    #################################################Emdbedding#####################################################\n",
    "    ## 频率与word_embedding\n",
    "    def get_coefs(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def merge_sentence_words(self, sentence_words):\n",
    "        result = []\n",
    "        for sentence in sentence_words:\n",
    "            result += sentence\n",
    "        return result\n",
    "\n",
    "    def get_frequence(self, word_sentence_content):\n",
    "        tokeners = self.merge_sentence_words(word_sentence_content)\n",
    "\n",
    "        tokener_counter = Counter(tokeners)\n",
    "\n",
    "        self.frequence = {w: count/len(tokeners) for w, count in tokener_counter.items()}\n",
    "        ## occurences_frequences = sorted(frequence.items(), key=lambda x: x[1], reverse=True)\n",
    "        pass\n",
    "    \n",
    "    def get_embedding_index(self):\n",
    "        self.embeddings_index = dict(self.get_coefs(*o.rstrip().rsplit(' ')) for o in open(self.embeddings_index_filename, encoding='UTF-8'))\n",
    "        pass\n",
    "    \n",
    "    ## 建立节点图\n",
    "    def get_connect_graph_by_text_rank(self, tokenized_text: str, window=3):\n",
    "        keywords_graph = networkx.Graph()\n",
    "        tokeners = tokenized_text.split()\n",
    "        for ii, t in enumerate(tokeners):\n",
    "            word_tuples = [(tokeners[connect], t) \n",
    "                           for connect in range(ii-window, ii+window+1) \n",
    "                           if connect >= 0 and connect < len(tokeners)]\n",
    "            keywords_graph.add_edges_from(word_tuples)\n",
    "\n",
    "        return keywords_graph\n",
    "    \n",
    "    ## 获取句子向量\n",
    "    def sentence_embedding(self, words):\n",
    "        #weight = alpah/(alpah + p)\n",
    "        #alpah is a parameter, 1e-3 ~ 1e-5\n",
    "        max_fre = max(self.frequence.values())\n",
    "\n",
    "        sentence_vec = np.zeros_like(self.embeddings_index['测试'])\n",
    "\n",
    "        words = [w for w in words if w in self.embeddings_index]\n",
    "        if words == []:\n",
    "            return sentence_vec\n",
    "\n",
    "        for w in words:\n",
    "            weight = self.alpah / (self.alpah + self.frequence.get(w, max_fre))\n",
    "            sentence_vec += weight * self.embeddings_index[w]\n",
    "\n",
    "        sentence_vec /= len(words)\n",
    "\n",
    "        return sentence_vec\n",
    "    \n",
    "    ################################################Emdbedding#####################################################\n",
    "    \n",
    "    ################################################KNN Smooth#####################################################\n",
    "    \n",
    "    def knn_smooth(self, unranking_sentence):\n",
    "        \n",
    "        window = self.knn_window\n",
    "        \n",
    "        sen = []\n",
    "        score = []\n",
    "        length = len(unranking_sentence)\n",
    "        for _sen, _score in unranking_sentence.items():\n",
    "            sen.append(_sen)\n",
    "            score.append(_score)\n",
    "        smooth_sentence = [np.mean(score[:i+window+1]) for i in range(window)] + [np.mean(score[i-window:i+window+1]) for i in range(window, length-window)] + [np.mean(score[i-window:]) for i in range(length-window, length)]\n",
    "\n",
    "        smooth_sentence_dict = {}\n",
    "        for i in range(length):\n",
    "            smooth_sentence_dict[sen[i]] = smooth_sentence[i]\n",
    "        print(smooth_sentence_dict)\n",
    "        print('\\n')\n",
    "        return sorted(smooth_sentence_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    ################################################KNN Smooth#####################################################\n",
    "    \n",
    "    #################################################LDA Model#####################################################\n",
    "    def get_LDA(self, sentence_words_content):\n",
    "        lda_load = models.ldamodel.LdaModel.load(self.lda_model_filename)\n",
    "        dictionary = lda_load.id2word\n",
    "\n",
    "        content = self.merge_sentence_words(sentence_words_content)\n",
    "        content_id = dictionary.doc2bow(content)\n",
    "\n",
    "        topic = lda_load.get_document_topics(content_id)\n",
    "        sorted_topic = sorted(topic, key=lambda x:x[1], reverse=True)\n",
    "        major_topic_num = sorted_topic[0][0]\n",
    "\n",
    "        print(\"第1封邮件的主题分布为：\\n\", sorted_topic, '\\n')\n",
    "\n",
    "        str_try = lda_load.print_topic(major_topic_num, topn=self.lda_num)\n",
    "\n",
    "        major_topic = re.findall('\"[\\u4e00-\\u9fa5].?\"', str_try)\n",
    "        for i in range(len(major_topic)):\n",
    "            major_topic[i] = major_topic[i][1:-1]\n",
    "\n",
    "    #     print(\"第1封邮件最重要的主题为：\\n\", major_topic, '\\n')\n",
    "        return major_topic\n",
    "    \n",
    "    #################################################LDA Model#####################################################\n",
    "    \n",
    "    #################################################Key Words#####################################################\n",
    "    class AttrDict(dict):\n",
    "        \"\"\"Dict that can get attribute by dot\"\"\"\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(AttrDict, self).__init__(*args, **kwargs)\n",
    "            self.__dict__ = self\n",
    "\n",
    "    def combine(self, word_list):\n",
    "        \"\"\"构造在window下的单词组合，用来构造单词之间的边。\n",
    "\n",
    "        Keyword arguments:\n",
    "        word_list  --  list of str, 由单词组成的列表。\n",
    "        windows    --  int, 窗口大小。\n",
    "        \"\"\"\n",
    "        window = self.keywords_window\n",
    "        \n",
    "        if window < 2: window = 2\n",
    "        for x in range(1, window):\n",
    "            if x >= len(word_list):\n",
    "                break\n",
    "            word_list2 = word_list[x:]\n",
    "            res = zip(word_list, word_list2)\n",
    "            for r in res:\n",
    "                yield r\n",
    "    \n",
    "    def get_word2id_id2word(self, sentence_words_content):\n",
    "        id2word = corpora.Dictionary(sentence_words_content)\n",
    "        word2id = {id2word[i]: i for i in range(len(id2word))}\n",
    "        return word2id, id2word\n",
    "\n",
    "    def creat_graph(self, sentence_words_content, word2id):\n",
    "        graph = np.zeros((len(word2id), len(word2id)))\n",
    "        for word_list in sentence_words_content:\n",
    "            for w1, w2 in self.combine(word_list):\n",
    "                if w1 in word2id and w2 in word2id:\n",
    "                    index1 = word2id[w1]\n",
    "                    index2 = word2id[w2]\n",
    "                    graph[index1][index2] = 1.0\n",
    "                    graph[index2][index1] = 1.0\n",
    "        return graph\n",
    "    \n",
    "    def get_keywords(self, sentence_words_content):\n",
    "        word2id, id2word = self.get_word2id_id2word(sentence_words_content)\n",
    "\n",
    "        graph = self.creat_graph(sentence_words_content, word2id)\n",
    "\n",
    "        sorted_words = []\n",
    "        nx_graph = nx.from_numpy_matrix(graph)\n",
    "        scores = nx.pagerank(nx_graph, alpha=0.85)          # this is a dict\n",
    "        sorted_scores = sorted(scores.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "        for index, score in sorted_scores:\n",
    "    #         item = AttrDict(word=id2word[index], weight=score)\n",
    "            sorted_words.append(id2word[index])\n",
    "\n",
    "        return sorted_words\n",
    "    \n",
    "    #################################################Key Words#####################################################\n",
    "    \n",
    "    #############################################Sentence Ranking##################################################\n",
    "    ## 获取句子节点排序_1：相邻句子词语共现频率\n",
    "    def sentence_ranking_by_text_ranking(self, split_sentence):\n",
    "        sentence_graph = self.get_connect_graph_by_text_rank(' '.join(split_sentence))\n",
    "        unranking_sentence = networkx.pagerank(sentence_graph)\n",
    "        return unranking_sentence\n",
    "    \n",
    "    ## 获取句子节点排序_2：句子向量相似度\n",
    "    def get_correlations(self, sentence_words_content, sentences_content, title_words_content):\n",
    "        #if isinstance(text, list): text = ' '.join(text)\n",
    "        \n",
    "        self.get_embedding_index()\n",
    "        text_words = self.merge_sentence_words(sentence_words_content)\n",
    "        all_sentence_vector = self.sentence_embedding(text_words)\n",
    "\n",
    "        title_vector = []\n",
    "        for title_sen in title_words_content:\n",
    "            title_vector.append(self.sentence_embedding(title_sen))\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        key_words = self.get_keywords(sentence_words_content)[:self.keywords_num]\n",
    "        print(key_words)\n",
    "        print('\\n')\n",
    "\n",
    "        topic_words = self.get_LDA(sentence_words_content)\n",
    "        print(topic_words)\n",
    "        print('\\n')\n",
    "        for i, sentence in enumerate(sentence_words_content):\n",
    "            vec = self.sentence_embedding(sentence)\n",
    "\n",
    "            if (vec == np.zeros_like(self.embeddings_index['测试'])).all():\n",
    "                correlations[sentences_content[i]] = 0.0\n",
    "                continue\n",
    "\n",
    "            correlation = (1 - self.title_weight) * cosine(all_sentence_vector, vec) + self.title_weight * sum([cosine(ti_vec, vec) for ti_vec in title_vector])/len(sentence_words_content)\n",
    "            correlations[sentences_content[i]] = correlation\n",
    "\n",
    "            key_num = 0 \n",
    "            topic_num = 0\n",
    "            for word in sentence:\n",
    "                if word in key_words:\n",
    "                    correlations[sentences_content[i]] *= 1.1\n",
    "    #                 key_num += 1 \n",
    "                if word in topic_words:\n",
    "                    topic_num += 1             \n",
    "    #         if topic_num == 0:\n",
    "    #             correlations[sentences_content[i]] *= 0.9\n",
    "            if topic_num != 0:\n",
    "                correlations[sentences_content[i]] *= 1.1\n",
    "\n",
    "        correlations[sentences_content[0]] *= 2.0\n",
    "        correlations[sentences_content[len(sentence_words_content) - 1]] *= 1.5\n",
    "\n",
    "        return correlations\n",
    "    #############################################Sentence Ranking##################################################\n",
    "    \n",
    "    ################################################## Main #######################################################   \n",
    "    def get_summarization_simple(self, text, title):\n",
    "        print(text)\n",
    "        print('\\n')\n",
    "        print(\"文章标题：    %s\"%title)\n",
    "        \n",
    "        score_fn = partial(self.get_correlations)\n",
    "        \n",
    "        sentence_words_content, sentences_content, _ = self.data_process(text)\n",
    "\n",
    "        self.get_frequence(sentence_words_content)\n",
    "\n",
    "        title_words_content, _, _ = self.data_process(title)\n",
    "\n",
    "        #print(title_words_content)\n",
    "        unranking_sentence = score_fn(sentence_words_content, sentences_content, title_words_content)\n",
    "        selected_text = set()\n",
    "        current_text = ''\n",
    "\n",
    "        ranking_sentence = self.knn_smooth(unranking_sentence)\n",
    "    #     ranking_sentence = sorted(unranking_sentence.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for sen, _ in ranking_sentence:\n",
    "            if len(current_text) < self.long_constraint:\n",
    "                current_text += sen\n",
    "                selected_text.add(sen)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        summarized = []\n",
    "        for sen in sentences_content:  # print the selected sentence by sequent\n",
    "            if sen in selected_text:\n",
    "                summarized.append(sen)\n",
    "        return ''.join(summarized)\n",
    "    \n",
    "    ## 最终函数_1: TextRank\n",
    "    def get_summarization_simple_with_text_rank(self, text):\n",
    "        return self.get_summarization_simple(text)\n",
    "\n",
    "    ## 最终函数_2: EmbeddingRank\n",
    "    def get_summarization_simple_by_sen_embedding(self, text, title):\n",
    "        return self.get_summarization_simple(text, title)\n",
    "    \n",
    "    ################################################## Main #######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summary_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = '7月11日，连续强降雨，让四川登上了中央气象台“头条”，涪江绵阳段水位迅速上涨，洪水一度漫过了宝成铁路涪江大桥桥墩基座，超过封锁水位。洪水在即，中国铁路成都局集团公司紧急调集两列重载货物列车，一前一后开上涪江大桥，每一列货车重量约四千吨，用“重车压梁”的方式，增强桥梁自重，抵御汹涌的洪水。从11日凌晨开始，四川境内成都、绵阳、广元等地连续强降雨，而四川北向出川大动脉—宝成铁路，便主要途径成绵广这一区域。连续的强降雨天气下，绵阳市境内的涪江水位迅速上涨，一度危及到了宝成铁路涪江大桥的安全，上午10时，水位已经超过了涪江大桥上、下行大桥的封锁水位。记者从中国铁路成都局集团公司绵阳工务段了解到，上行线涪江大桥，全长393米，建成于1953年；下行线涪江大桥，全长438米，建成于1995年。“涪江大桥上游有一个水电站，由于洪水太大，水电站已无法发挥调节水位的作用。”情况紧急，铁路部门决定采用“重车压梁”的方式，增强桥梁自重，提高洪峰对桥墩冲刷时的梁体稳定性。简单来说，就是将重量大的货物列车开上涪江大桥，用货车的自重，帮助桥梁抵御汹涌的洪水。恰好，绵阳工务段近期正在进行线路大修，铁路专用的卸砟车，正好停放在绵阳附近。迎着汹涌的洪水，两列重载货车驶向宝成铁路涪江大桥。上午10时30分，第一列46052次货车，从绵阳北站开出进入上行涪江桥。上午11时15分，第二列22001次货车，从皂角铺站进入下行涪江桥。这是两列超过45节编组的重载货物列车，业内称铁路专用卸砟车，俗称“老K车”，车厢里装载的均为铁路道砟，每辆车的砟石的重量在70吨左右。记者从绵阳工务段了解到，货车里满载的砟石、加上一列货车的自重，两列“压桥”的货运列车，每一列的重量超过四千吨。“采用重车压梁的方式来应对水害，在平时的抢险中很少用到。”据了解，在绵阳工务段，上一次采用重车压梁，至少已经是二十年前的事。下午4时许，经铁路部门观测，洪峰过后，涪江水位开始下降，目前已经低于桥梁封锁水位。从下午4点37分开始，两列火车开始撤离涪江大桥。在桥上停留约6个小时后，两列重载货物列车成功完成了“保桥任务”，宝成铁路涪江大桥平安了！'\n",
    "test_title_1 = '20年来首次！四千吨级重载货车压桥抵御洪峰，宝成铁路大桥已平安。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7月11日，连续强降雨，让四川登上了中央气象台“头条”，涪江绵阳段水位迅速上涨，洪水一度漫过了宝成铁路涪江大桥桥墩基座，超过封锁水位。洪水在即，中国铁路成都局集团公司紧急调集两列重载货物列车，一前一后开上涪江大桥，每一列货车重量约四千吨，用“重车压梁”的方式，增强桥梁自重，抵御汹涌的洪水。从11日凌晨开始，四川境内成都、绵阳、广元等地连续强降雨，而四川北向出川大动脉—宝成铁路，便主要途径成绵广这一区域。连续的强降雨天气下，绵阳市境内的涪江水位迅速上涨，一度危及到了宝成铁路涪江大桥的安全，上午10时，水位已经超过了涪江大桥上、下行大桥的封锁水位。记者从中国铁路成都局集团公司绵阳工务段了解到，上行线涪江大桥，全长393米，建成于1953年；下行线涪江大桥，全长438米，建成于1995年。“涪江大桥上游有一个水电站，由于洪水太大，水电站已无法发挥调节水位的作用。”情况紧急，铁路部门决定采用“重车压梁”的方式，增强桥梁自重，提高洪峰对桥墩冲刷时的梁体稳定性。简单来说，就是将重量大的货物列车开上涪江大桥，用货车的自重，帮助桥梁抵御汹涌的洪水。恰好，绵阳工务段近期正在进行线路大修，铁路专用的卸砟车，正好停放在绵阳附近。迎着汹涌的洪水，两列重载货车驶向宝成铁路涪江大桥。上午10时30分，第一列46052次货车，从绵阳北站开出进入上行涪江桥。上午11时15分，第二列22001次货车，从皂角铺站进入下行涪江桥。这是两列超过45节编组的重载货物列车，业内称铁路专用卸砟车，俗称“老K车”，车厢里装载的均为铁路道砟，每辆车的砟石的重量在70吨左右。记者从绵阳工务段了解到，货车里满载的砟石、加上一列货车的自重，两列“压桥”的货运列车，每一列的重量超过四千吨。“采用重车压梁的方式来应对水害，在平时的抢险中很少用到。”据了解，在绵阳工务段，上一次采用重车压梁，至少已经是二十年前的事。下午4时许，经铁路部门观测，洪峰过后，涪江水位开始下降，目前已经低于桥梁封锁水位。从下午4点37分开始，两列火车开始撤离涪江大桥。在桥上停留约6个小时后，两列重载货物列车成功完成了“保桥任务”，宝成铁路涪江大桥平安了！\n",
      "\n",
      "\n",
      "文章标题：    20年来首次！四千吨级重载货车压桥抵御洪峰，宝成铁路大桥已平安。\n",
      "['涪江', '绵阳', '大桥', '铁路', '两列']\n",
      "\n",
      "\n",
      "第1封邮件的主题分布为：\n",
      " [(12, 0.21200399), (0, 0.13769695), (15, 0.12731786), (17, 0.11685065), (10, 0.097226106), (1, 0.067340046), (5, 0.05304143), (3, 0.043972187), (4, 0.043836355), (14, 0.029150225), (2, 0.019577263), (13, 0.019342441), (18, 0.014616629), (19, 0.0134514775)] \n",
      "\n",
      "['发生', '警方', '斋月', '死亡', '名', '造成', '一名', '袭击', '记者', '事件', '受伤', '人员', '现场', '爆炸', '至少', '医院', '目前', '附近', '当地', '事故', '伦敦', '警察', '救援', '恐袭', '男子', '儿童', '正在', '日晚', '地铁', '调查', '一起', '发现', '一辆', '时间', '车辆', '进行', '岁', '列车', '严重', '汽车', '导致', '下午', '圣', '民警', '英国', '手术', '组织', '乘客', '日电', '凌晨', '驾驶', '一个', '上午', '工作', '全部', '安全', '救助', '医生', '报道', '司机', '行人', '浜', '紧急', '小时', '酒店', '左右', '仍', '接受', '随后', '患儿', '两名', '广场', '炸弹', '危险', '部门', '期间', '完', '治疗', '当天', '击毙', '犯罪', '枪手', '火灾']\n",
      "\n",
      "\n",
      "{'7月11日，连续强降雨，让四川登上了中央气象台“头条”，涪江绵阳段水位迅速上涨，洪水一度漫过了宝成铁路涪江大桥桥墩基座，超过封锁水位。': 0.20172550629730154, '洪水在即，中国铁路成都局集团公司紧急调集两列重载货物列车，一前一后开上涪江大桥，每一列货车重量约四千吨，用“重车压梁”的方式，增强桥梁自重，抵御汹涌的洪水。': 0.17889437986102455, '从11日凌晨开始，四川境内成都、绵阳、广元等地连续强降雨，而四川北向出川大动脉—宝成铁路，便主要途径成绵广这一区域。': 0.13448896092481608, '连续的强降雨天气下，绵阳市境内的涪江水位迅速上涨，一度危及到了宝成铁路涪江大桥的安全，上午10时，水位已经超过了涪江大桥上、下行大桥的封锁水位。': 0.15827300691737928, '记者从中国铁路成都局集团公司绵阳工务段了解到，上行线涪江大桥，全长393米，建成于1953年；': 0.17656324482345775, '下行线涪江大桥，全长438米，建成于1995年。': 0.18236756702069198, '“涪江大桥上游有一个水电站，由于洪水太大，水电站已无法发挥调节水位的作用。”': 0.16260322239349287, '情况紧急，铁路部门决定采用“重车压梁”的方式，增强桥梁自重，提高洪峰对桥墩冲刷时的梁体稳定性。': 0.15123322781672083, '简单来说，就是将重量大的货物列车开上涪江大桥，用货车的自重，帮助桥梁抵御汹涌的洪水。': 0.13784450940022872, '恰好，绵阳工务段近期正在进行线路大修，铁路专用的卸砟车，正好停放在绵阳附近。': 0.16572157464131715, '迎着汹涌的洪水，两列重载货车驶向宝成铁路涪江大桥。': 0.17882350877354544, '上午10时30分，第一列46052次货车，从绵阳北站开出进入上行涪江桥。': 0.19220700576504077, '上午11时15分，第二列22001次货车，从皂角铺站进入下行涪江桥。': 0.16770053110649194, '这是两列超过45节编组的重载货物列车，业内称铁路专用卸砟车，俗称“老K车”，车厢里装载的均为铁路道砟，每辆车的砟石的重量在70吨左右。': 0.15238844903613133, '记者从绵阳工务段了解到，货车里满载的砟石、加上一列货车的自重，两列“压桥”的货运列车，每一列的重量超过四千吨。': 0.13880851663703722, '“采用重车压梁的方式来应对水害，在平时的抢险中很少用到。”': 0.15632475775033236, '据了解，在绵阳工务段，上一次采用重车压梁，至少已经是二十年前的事。': 0.15106177248954775, '下午4时许，经铁路部门观测，洪峰过后，涪江水位开始下降，目前已经低于桥梁封锁水位。': 0.17857481879850232, '从下午4点37分开始，两列火车开始撤离涪江大桥。': 0.20730749596716222, '在桥上停留约6个小时后，两列重载货物列车成功完成了“保桥任务”，宝成铁路涪江大桥平安了！': 0.2451745152609237}\n",
      "\n",
      "\n",
      "7月11日，连续强降雨，让四川登上了中央气象台“头条”，涪江绵阳段水位迅速上涨，洪水一度漫过了宝成铁路涪江大桥桥墩基座，超过封锁水位。洪水在即，中国铁路成都局集团公司紧急调集两列重载货物列车，一前一后开上涪江大桥，每一列货车重量约四千吨，用“重车压梁”的方式，增强桥梁自重，抵御汹涌的洪水。下行线涪江大桥，全长438米，建成于1995年。上午10时30分，第一列46052次货车，从绵阳北站开出进入上行涪江桥。从下午4点37分开始，两列火车开始撤离涪江大桥。在桥上停留约6个小时后，两列重载货物列车成功完成了“保桥任务”，宝成铁路涪江大桥平安了！\n"
     ]
    }
   ],
   "source": [
    "print(model.get_summarization_simple_by_sen_embedding(test_1, test_title_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_2 = '因和美国总统特朗普意见不一，美国情报界2号人物被踢出局！当地时间8日晚间，特朗普连发两条推特，先是解雇了现任美国国家情报副总监苏·戈登，他说：“苏·戈登是一位了不起的专家，有着漫长而杰出的职业生涯。在过去两年中，我认识了苏，并对她产生了极大的尊重。苏已经宣布她将于8月15日离职，这一天正好是丹·科茨退休的日子。不久我将任命一位新的国家情报代理总监。”紧接着，特朗普又抛出一条更具份量的推特：“很高兴向你们宣布，现国家反恐中心主任约瑟夫·马奎尔从8月15日起将担任国家情报代理总监。”按理说，特朗普作为总统，任命政府部门的主管本是“分内事”。再者，特朗普发推特换个部门主管也不是一回两回了，但让美国政界和舆论界吃惊的是，这次任命的背后大有乾坤。美国有线电视新闻网（CNN）9日称，特朗普与美国情报界积怨已久，这次仅提前一星期宣布戈登“辞职”，很可能意味着他要把本应远离政治的情报界拉进党派斗争的泥潭。美国国家情报副总监戈登在给特朗普的辞职信中写道：“因为您要求一个新的领导班子掌舵，所以我愿意于2019年8月15日辞去职务。”“我相信我们的团队拥有力量，他们不会让你和我们的国家失望。”虽说戈登字里行间似乎写得情真意切，但明眼人一看便知，戈登是在暗示：她是被逼走的。据美国媒体报道，戈登是美国情报界的元老级人物，在业内服务长达30多年，位居美国国家情报局的二把手，只是因为不顺特朗普的心，就遭到“被辞职”的命运。据美国“政治”网站8日报道，戈登曾这样评价情报机构与特朗普的关系：“我们试图让他满意，就像过去每一任总统那样，但实际上，我们这群向他报告世界正如何变化的人才是让他心情不好的原因，他对我们的工作认同与否变得一点也不重要。”同时，戈登在情报界是出了名的“无党派老黄牛”，任劳任怨，深受两党欢迎，这不得不让人担心，万一出了差错，指责可能都是冲着她“老板”来的。上个月，美国国家情报总监丹·科茨宣布即将退休。按规定，戈登应该担任代理总监，但白宫就是不允，最后选了国家反恐中心主任马奎尔担任这一职务。对于特朗普的举动，美国国会方面很不买账。众议院情报委员会主席亚当·希夫明确批评说，“这些优秀领导的去职，以及总统决心要清理任何胆敢有不同意见的人，显而易见是情报界所面临的最具挑战性的时刻”。说来也巧，美国联邦调查局前副局长安德鲁·麦凯布2018年1月眼看要退休了，却被司法部无情解雇。8日，麦凯布将司法部和联邦调查局告上法庭，指称特朗普出于“政治动机”迫使司法部解雇了他，认为这是出于“党派偏见”的“违宪行为”。不管这场官司最后结果如何，特朗普多么不能容人无疑又会在公众面前好好晒一遍。自从特朗普2017年1月入主白宫后，政府人事变动犹如“走马灯”一般。从国务卿、国防部长、司法部长、白宫办公室主任、驻联合国大使等等，换了一个又一个，绝大多数是因与特朗普不同“调”。现任国务卿蓬佩奥在一次聚会上也无奈地说，说不定哪一天，他就在总统的推特上被解雇了。有分析认为，特朗普就是一根筋，眼里容不得一粒沙子。其上任以来，先是大骂媒体制造假新闻，只有他的推特才是真新闻。对于朝鲜半岛核问题、“伊斯兰国”的威胁、阿富汗局势以及伊朗核问题等等，情报官员给出的中肯意见，特朗普都不屑一顾。如今他对情报界开刀，顺之则留，逆之则去，将是这位总统的“不二法则”。'\n",
    "test_title_2 = '美情报界二号人物“被辞职”，美媒猜原因：总让特朗普“心情不好”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "因和美国总统特朗普意见不一，美国情报界2号人物被踢出局！当地时间8日晚间，特朗普连发两条推特，先是解雇了现任美国国家情报副总监苏·戈登，他说：“苏·戈登是一位了不起的专家，有着漫长而杰出的职业生涯。在过去两年中，我认识了苏，并对她产生了极大的尊重。苏已经宣布她将于8月15日离职，这一天正好是丹·科茨退休的日子。不久我将任命一位新的国家情报代理总监。”紧接着，特朗普又抛出一条更具份量的推特：“很高兴向你们宣布，现国家反恐中心主任约瑟夫·马奎尔从8月15日起将担任国家情报代理总监。”按理说，特朗普作为总统，任命政府部门的主管本是“分内事”。再者，特朗普发推特换个部门主管也不是一回两回了，但让美国政界和舆论界吃惊的是，这次任命的背后大有乾坤。美国有线电视新闻网（CNN）9日称，特朗普与美国情报界积怨已久，这次仅提前一星期宣布戈登“辞职”，很可能意味着他要把本应远离政治的情报界拉进党派斗争的泥潭。美国国家情报副总监戈登在给特朗普的辞职信中写道：“因为您要求一个新的领导班子掌舵，所以我愿意于2019年8月15日辞去职务。”“我相信我们的团队拥有力量，他们不会让你和我们的国家失望。”虽说戈登字里行间似乎写得情真意切，但明眼人一看便知，戈登是在暗示：她是被逼走的。据美国媒体报道，戈登是美国情报界的元老级人物，在业内服务长达30多年，位居美国国家情报局的二把手，只是因为不顺特朗普的心，就遭到“被辞职”的命运。据美国“政治”网站8日报道，戈登曾这样评价情报机构与特朗普的关系：“我们试图让他满意，就像过去每一任总统那样，但实际上，我们这群向他报告世界正如何变化的人才是让他心情不好的原因，他对我们的工作认同与否变得一点也不重要。”同时，戈登在情报界是出了名的“无党派老黄牛”，任劳任怨，深受两党欢迎，这不得不让人担心，万一出了差错，指责可能都是冲着她“老板”来的。上个月，美国国家情报总监丹·科茨宣布即将退休。按规定，戈登应该担任代理总监，但白宫就是不允，最后选了国家反恐中心主任马奎尔担任这一职务。对于特朗普的举动，美国国会方面很不买账。众议院情报委员会主席亚当·希夫明确批评说，“这些优秀领导的去职，以及总统决心要清理任何胆敢有不同意见的人，显而易见是情报界所面临的最具挑战性的时刻”。说来也巧，美国联邦调查局前副局长安德鲁·麦凯布2018年1月眼看要退休了，却被司法部无情解雇。8日，麦凯布将司法部和联邦调查局告上法庭，指称特朗普出于“政治动机”迫使司法部解雇了他，认为这是出于“党派偏见”的“违宪行为”。不管这场官司最后结果如何，特朗普多么不能容人无疑又会在公众面前好好晒一遍。自从特朗普2017年1月入主白宫后，政府人事变动犹如“走马灯”一般。从国务卿、国防部长、司法部长、白宫办公室主任、驻联合国大使等等，换了一个又一个，绝大多数是因与特朗普不同“调”。现任国务卿蓬佩奥在一次聚会上也无奈地说，说不定哪一天，他就在总统的推特上被解雇了。有分析认为，特朗普就是一根筋，眼里容不得一粒沙子。其上任以来，先是大骂媒体制造假新闻，只有他的推特才是真新闻。对于朝鲜半岛核问题、“伊斯兰国”的威胁、阿富汗局势以及伊朗核问题等等，情报官员给出的中肯意见，特朗普都不屑一顾。如今他对情报界开刀，顺之则留，逆之则去，将是这位总统的“不二法则”。\n",
      "\n",
      "\n",
      "文章标题：    美情报界二号人物“被辞职”，美媒猜原因：总让特朗普“心情不好”\n",
      "['特朗普', '美国', '情报界', '总统', '戈登']\n",
      "\n",
      "\n",
      "第1封邮件的主题分布为：\n",
      " [(7, 0.5120175), (1, 0.2902052), (15, 0.04197935), (3, 0.040893834), (4, 0.035307847), (10, 0.025033982), (19, 0.010158852), (0, 0.010078569)] \n",
      "\n",
      "['法网', '美国', '表示', '总统', '政府', '国家', '组织', '问题', '宣布', '协定', '日本', '国', '报道', '国际', '安全', '认为', '地区', '当天', '巴黎', '媒体', '记者', '欧盟', '武装', '沙特', '举行', '可能', '日电', '俄', '进行', '关系', '协议', '英国', '支持', '埃及', '伊朗', '没有', '继续', '总理', '韩国', '退出', '拉扎', '调查', '断交', '完', '应对', '声明', '首都', '包括', '北约', '方面', '决定', '简讯', '德国', '巴西', '和平', '民众', '官员', '双方', '打击', '会晤', '这一', '朝鲜', '普京', '接受', '袭击', '反恐', '政治', '访问', '目前', '发表', '要求', '极端', '事件', '行动', '已经', '政策', '达成', '白宫', '波尔', '驻']\n",
      "\n",
      "\n",
      "{'因和美国总统特朗普意见不一，美国情报界2号人物被踢出局！': 0.3583958652730628, '当地时间8日晚间，特朗普连发两条推特，先是解雇了现任美国国家情报副总监苏·戈登，他说：“苏·戈登是一位了不起的专家，有着漫长而杰出的职业生涯。': 0.2828484835441073, '在过去两年中，我认识了苏，并对她产生了极大的尊重。': 0.12622758852407853, '苏已经宣布她将于8月15日离职，这一天正好是丹·科茨退休的日子。': 0.1527609902209249, '不久我将任命一位新的国家情报代理总监。”': 0.13851728830186802, '紧接着，特朗普又抛出一条更具份量的推特：“很高兴向你们宣布，现国家反恐中心主任约瑟夫·马奎尔从8月15日起将担任国家情报代理总监。”': 0.13574620022650424, '按理说，特朗普作为总统，任命政府部门的主管本是“分内事”。': 0.10198142306352487, '再者，特朗普发推特换个部门主管也不是一回两回了，但让美国政界和舆论界吃惊的是，这次任命的背后大有乾坤。': 0.10889076260108684, '美国有线电视新闻网（CNN）9日称，特朗普与美国情报界积怨已久，这次仅提前一星期宣布戈登“辞职”，很可能意味着他要把本应远离政治的情报界拉进党派斗争的泥潭。': 0.10577155981118279, '美国国家情报副总监戈登在给特朗普的辞职信中写道：“因为您要求一个新的领导班子掌舵，所以我愿意于2019年8月15日辞去职务。”': 0.12030785315200827, '“我相信我们的团队拥有力量，他们不会让你和我们的国家失望。”': 0.13232332769687152, '虽说戈登字里行间似乎写得情真意切，但明眼人一看便知，戈登是在暗示：她是被逼走的。': 0.1345033943802714, '据美国媒体报道，戈登是美国情报界的元老级人物，在业内服务长达30多年，位居美国国家情报局的二把手，只是因为不顺特朗普的心，就遭到“被辞职”的命运。': 0.11904301751513384, '据美国“政治”网站8日报道，戈登曾这样评价情报机构与特朗普的关系：“我们试图让他满意，就像过去每一任总统那样，但实际上，我们这群向他报告世界正如何变化的人才是让他心情不好的原因，他对我们的工作认同与否变得一点也不重要。”': 0.10388540965384245, '同时，戈登在情报界是出了名的“无党派老黄牛”，任劳任怨，深受两党欢迎，这不得不让人担心，万一出了差错，指责可能都是冲着她“老板”来的。': 0.1137181610130168, '上个月，美国国家情报总监丹·科茨宣布即将退休。': 0.1214267692836537, '按规定，戈登应该担任代理总监，但白宫就是不允，最后选了国家反恐中心主任马奎尔担任这一职务。': 0.12547426184018454, '对于特朗普的举动，美国国会方面很不买账。': 0.09566720655252196, '众议院情报委员会主席亚当·希夫明确批评说，“这些优秀领导的去职，以及总统决心要清理任何胆敢有不同意见的人，显而易见是情报界所面临的最具挑战性的时刻”。': 0.09015985381671754, '说来也巧，美国联邦调查局前副局长安德鲁·麦凯布2018年1月眼看要退休了，却被司法部无情解雇。': 0.09004646702401942, '8日，麦凯布将司法部和联邦调查局告上法庭，指称特朗普出于“政治动机”迫使司法部解雇了他，认为这是出于“党派偏见”的“违宪行为”。': 0.10159819389896833, '不管这场官司最后结果如何，特朗普多么不能容人无疑又会在公众面前好好晒一遍。': 0.12487666906639079, '自从特朗普2017年1月入主白宫后，政府人事变动犹如“走马灯”一般。': 0.13863373299097195, '从国务卿、国防部长、司法部长、白宫办公室主任、驻联合国大使等等，换了一个又一个，绝大多数是因与特朗普不同“调”。': 0.1476720505678791, '现任国务卿蓬佩奥在一次聚会上也无奈地说，说不定哪一天，他就在总统的推特上被解雇了。': 0.15736510886200547, '有分析认为，特朗普就是一根筋，眼里容不得一粒沙子。': 0.14458536034000333, '其上任以来，先是大骂媒体制造假新闻，只有他的推特才是真新闻。': 0.1384206313888232, '对于朝鲜半岛核问题、“伊斯兰国”的威胁、阿富汗局势以及伊朗核问题等等，情报官员给出的中肯意见，特朗普都不屑一顾。': 0.16222453844358867, '如今他对情报界开刀，顺之则留，逆之则去，将是这位总统的“不二法则”。': 0.1858075594195518}\n",
      "\n",
      "\n",
      "因和美国总统特朗普意见不一，美国情报界2号人物被踢出局！当地时间8日晚间，特朗普连发两条推特，先是解雇了现任美国国家情报副总监苏·戈登，他说：“苏·戈登是一位了不起的专家，有着漫长而杰出的职业生涯。现任国务卿蓬佩奥在一次聚会上也无奈地说，说不定哪一天，他就在总统的推特上被解雇了。对于朝鲜半岛核问题、“伊斯兰国”的威胁、阿富汗局势以及伊朗核问题等等，情报官员给出的中肯意见，特朗普都不屑一顾。如今他对情报界开刀，顺之则留，逆之则去，将是这位总统的“不二法则”。\n"
     ]
    }
   ],
   "source": [
    "#get_summarization_simple_by_sen_embedding(test_1, test_title_1, frequence, embeddings_index, constraint=200)\n",
    "print(model.get_summarization_simple_by_sen_embedding(test_2, test_title_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = '参考消息网8月15日报道 英国《卫报》网站8月11日发表题为《为何特朗普的“极限施压”外交政策收效甚微？》的文章，作者为朱利安·博格。文章称，唐纳德·特朗普即将带着没有明显外交胜利的政绩投入2020年大选。一些戏剧性的失败和世界各地一系列迫在眉睫的危机可能会破坏他争取连任的努力。因此，许多人预测，特朗普将寻求通过在全球采取影响深远但结果难料的干预行动来扭转这一趋势——这将使未来16个月的局势比他担任总统以来更为动荡。文章称，外交政策问题很少在美国总统大选中发挥主导作用，但特朗普却不遗余力地将自己塑造为能与外国领导人达成协议、并将美军撤回国内的唯一人选。一些外国危机对美国国内产生了直接影响。特朗普承诺的中美协议至今没有达成，加征关税和贸易战愈演愈烈，这些都将直接影响到消费者和生产商。奥巴马的演讲撰稿人、外交政策顾问本·罗兹说：“有意思的是，在这其中的每一个问题上，特朗普都看到了在国内政治中可以带来的好处：对中国、伊朗和委内瑞拉强硬，心中想的是争取佛罗里达选民……但无论短期利益如何，都已被他制造的长期混乱蒙上了阴影。”文章认为，即将全面开启竞选模式的特朗普似乎意识到了自己在外交政策上的不足，并寻求解决这个问题。从短期来看，这意味着重新定义成功。文章称，在朝鲜问题上，这意味着淡化他最初的说法，即与朝鲜的外交最终将带来核裁军。一直以来，他强调的是朝鲜暂停核试验和导弹试验。当朝鲜加紧发射短程导弹时，红线就被移至防止其发射洲际导弹上。文章还称，针对伊朗的“极限施压”战略旨在改变伊朗政府在中东地区的行为方式，结束其铀浓缩活动，使其放弃直接或通过代理人介入叙利亚、伊拉克和也门事务，并说服伊朗与美国开启新的谈判。以这些标准来衡量，到目前为止，这项战略正在产生事与愿违的效果。伊朗正在突破2015年核协议的限制，加紧推进核计划。目前尚不清楚，随着朝鲜越来越多地进行的导弹发射，以及伊朗突破核限制，降低成功门槛的战术能否使特朗普在明年11月的大选中获胜。文章称，特朗普的另一个选择是，采取大胆的行动重新掌握主动权。这将可能是军事形式，但也是最后的手段。无论对选举还是对其他方面而言，朝鲜半岛或波斯湾再发生冲突将是灾难性的。今年6月，特朗普打击伊朗的行动“已经箭在弦上”，却在最后一刻取消了行动。当时有人警告他说，这将可能使他失去第二个任期。文章还称，他本能地想要做一笔引人注目的交易。他表示有意与朝鲜领导人再次举行峰会。在峰会上，他可以提出以拆除一些朝鲜核武器基础设施为条件，取消部分制裁措施。迄今为止，特朗普寻求与伊朗领导人直接对话的努力都遭到拒绝，但9月的联合国大会提供了另一个与伊朗总统哈桑·鲁哈尼会晤的机会。要做到这一点，就可能需要极大缓解对伊朗的经济施压。特朗普也许认为值得冒这个险，即使这需要他的极端鹰派国家安全顾问约翰·博尔顿下台。文章指出，中美也可能达成部分贸易协议。特朗普将可能利用任何这样的协议“大做文章”，称其为世纪性的协议。曾在奥巴马政府担任高级国家安全官员的朱莉安娜·史密斯说：“特朗普将抢占头条，将自己塑造成领袖。”她说：“未来18个月，我们将看到一系列轻率和鲁莽的举动以及更多的戏剧性事件。所以，坐稳了！”'\n",
    "test_title_3 = '英媒预测特朗普或制造“大交易”提升连任几率'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考消息网8月15日报道 英国《卫报》网站8月11日发表题为《为何特朗普的“极限施压”外交政策收效甚微？》的文章，作者为朱利安·博格。文章称，唐纳德·特朗普即将带着没有明显外交胜利的政绩投入2020年大选。一些戏剧性的失败和世界各地一系列迫在眉睫的危机可能会破坏他争取连任的努力。因此，许多人预测，特朗普将寻求通过在全球采取影响深远但结果难料的干预行动来扭转这一趋势——这将使未来16个月的局势比他担任总统以来更为动荡。文章称，外交政策问题很少在美国总统大选中发挥主导作用，但特朗普却不遗余力地将自己塑造为能与外国领导人达成协议、并将美军撤回国内的唯一人选。一些外国危机对美国国内产生了直接影响。特朗普承诺的中美协议至今没有达成，加征关税和贸易战愈演愈烈，这些都将直接影响到消费者和生产商。奥巴马的演讲撰稿人、外交政策顾问本·罗兹说：“有意思的是，在这其中的每一个问题上，特朗普都看到了在国内政治中可以带来的好处：对中国、伊朗和委内瑞拉强硬，心中想的是争取佛罗里达选民……但无论短期利益如何，都已被他制造的长期混乱蒙上了阴影。”文章认为，即将全面开启竞选模式的特朗普似乎意识到了自己在外交政策上的不足，并寻求解决这个问题。从短期来看，这意味着重新定义成功。文章称，在朝鲜问题上，这意味着淡化他最初的说法，即与朝鲜的外交最终将带来核裁军。一直以来，他强调的是朝鲜暂停核试验和导弹试验。当朝鲜加紧发射短程导弹时，红线就被移至防止其发射洲际导弹上。文章还称，针对伊朗的“极限施压”战略旨在改变伊朗政府在中东地区的行为方式，结束其铀浓缩活动，使其放弃直接或通过代理人介入叙利亚、伊拉克和也门事务，并说服伊朗与美国开启新的谈判。以这些标准来衡量，到目前为止，这项战略正在产生事与愿违的效果。伊朗正在突破2015年核协议的限制，加紧推进核计划。目前尚不清楚，随着朝鲜越来越多地进行的导弹发射，以及伊朗突破核限制，降低成功门槛的战术能否使特朗普在明年11月的大选中获胜。文章称，特朗普的另一个选择是，采取大胆的行动重新掌握主动权。这将可能是军事形式，但也是最后的手段。无论对选举还是对其他方面而言，朝鲜半岛或波斯湾再发生冲突将是灾难性的。今年6月，特朗普打击伊朗的行动“已经箭在弦上”，却在最后一刻取消了行动。当时有人警告他说，这将可能使他失去第二个任期。文章还称，他本能地想要做一笔引人注目的交易。他表示有意与朝鲜领导人再次举行峰会。在峰会上，他可以提出以拆除一些朝鲜核武器基础设施为条件，取消部分制裁措施。迄今为止，特朗普寻求与伊朗领导人直接对话的努力都遭到拒绝，但9月的联合国大会提供了另一个与伊朗总统哈桑·鲁哈尼会晤的机会。要做到这一点，就可能需要极大缓解对伊朗的经济施压。特朗普也许认为值得冒这个险，即使这需要他的极端鹰派国家安全顾问约翰·博尔顿下台。文章指出，中美也可能达成部分贸易协议。特朗普将可能利用任何这样的协议“大做文章”，称其为世纪性的协议。曾在奥巴马政府担任高级国家安全官员的朱莉安娜·史密斯说：“特朗普将抢占头条，将自己塑造成领袖。”她说：“未来18个月，我们将看到一系列轻率和鲁莽的举动以及更多的戏剧性事件。所以，坐稳了！”\n",
      "\n",
      "\n",
      "文章标题：    英媒预测特朗普或制造“大交易”提升连任几率\n",
      "['特朗普', '伊朗', '朝鲜', '可能', '文章']\n",
      "\n",
      "\n",
      "第1封邮件的主题分布为：\n",
      " [(7, 0.59922975), (1, 0.12341403), (18, 0.04760871), (2, 0.04596843), (17, 0.04163919), (15, 0.03345713), (5, 0.03298915), (4, 0.025008801), (3, 0.01330781)] \n",
      "\n",
      "['法网', '美国', '表示', '总统', '政府', '国家', '组织', '问题', '宣布', '协定', '日本', '国', '报道', '国际', '安全', '认为', '地区', '当天', '巴黎', '媒体', '记者', '欧盟', '武装', '沙特', '举行', '可能', '日电', '俄', '进行', '关系', '协议', '英国', '支持', '埃及', '伊朗', '没有', '继续', '总理', '韩国', '退出', '拉扎', '调查', '断交', '完', '应对', '声明', '首都', '包括', '北约', '方面', '决定', '简讯', '德国', '巴西', '和平', '民众', '官员', '双方', '打击', '会晤', '这一', '朝鲜', '普京', '接受', '袭击', '反恐', '政治', '访问', '目前', '发表', '要求', '极端', '事件', '行动', '已经', '政策', '达成', '白宫', '波尔', '驻']\n",
      "\n",
      "\n",
      "{'参考消息网8月15日报道 英国《卫报》网站8月11日发表题为《为何特朗普的“极限施压”外交政策收效甚微？': 0.27089876772676197, '》的文章，作者为朱利安·博格。': 0.2194729622063183, '文章称，唐纳德·特朗普即将带着没有明显外交胜利的政绩投入2020年大选。': 0.1482243773312796, '一些戏剧性的失败和世界各地一系列迫在眉睫的危机可能会破坏他争取连任的努力。': 0.0919151302689598, '因此，许多人预测，特朗普将寻求通过在全球采取影响深远但结果难料的干预行动来扭转这一趋势——这将使未来16个月的局势比他担任总统以来更为动荡。': 0.07549842298484985, '文章称，外交政策问题很少在美国总统大选中发挥主导作用，但特朗普却不遗余力地将自己塑造为能与外国领导人达成协议、并将美军撤回国内的唯一人选。': 0.08352707626876378, '一些外国危机对美国国内产生了直接影响。': 0.09302978110483717, '特朗普承诺的中美协议至今没有达成，加征关税和贸易战愈演愈烈，这些都将直接影响到消费者和生产商。': 0.09941100029803458, '奥巴马的演讲撰稿人、外交政策顾问本·罗兹说：“有意思的是，在这其中的每一个问题上，特朗普都看到了在国内政治中可以带来的好处：对中国、伊朗和委内瑞拉强硬，心中想的是争取佛罗里达选民……': 0.10603769339351428, '但无论短期利益如何，都已被他制造的长期混乱蒙上了阴影。”': 0.09934390703184266, '文章认为，即将全面开启竞选模式的特朗普似乎意识到了自己在外交政策上的不足，并寻求解决这个问题。': 0.11149076877718882, '从短期来看，这意味着重新定义成功。': 0.10117052963469712, '文章称，在朝鲜问题上，这意味着淡化他最初的说法，即与朝鲜的外交最终将带来核裁军。': 0.11924008627854643, '一直以来，他强调的是朝鲜暂停核试验和导弹试验。': 0.13792581703955223, '当朝鲜加紧发射短程导弹时，红线就被移至防止其发射洲际导弹上。': 0.12511902672444072, '文章还称，针对伊朗的“极限施压”战略旨在改变伊朗政府在中东地区的行为方式，结束其铀浓缩活动，使其放弃直接或通过代理人介入叙利亚、伊拉克和也门事务，并说服伊朗与美国开启新的谈判。': 0.10904571306529502, '以这些标准来衡量，到目前为止，这项战略正在产生事与愿违的效果。': 0.0929093869594733, '伊朗正在突破2015年核协议的限制，加紧推进核计划。': 0.10320207811894873, '目前尚不清楚，随着朝鲜越来越多地进行的导弹发射，以及伊朗突破核限制，降低成功门槛的战术能否使特朗普在明年11月的大选中获胜。': 0.12106493987287797, '文章称，特朗普的另一个选择是，采取大胆的行动重新掌握主动权。': 0.1356736835277081, '这将可能是军事形式，但也是最后的手段。': 0.14431353358427687, '无论对选举还是对其他方面而言，朝鲜半岛或波斯湾再发生冲突将是灾难性的。': 0.1438700615196001, '今年6月，特朗普打击伊朗的行动“已经箭在弦上”，却在最后一刻取消了行动。': 0.13288852054874103, '当时有人警告他说，这将可能使他失去第二个任期。': 0.1480204357320354, '文章还称，他本能地想要做一笔引人注目的交易。': 0.15289637052161356, '他表示有意与朝鲜领导人再次举行峰会。': 0.14295611789680665, '在峰会上，他可以提出以拆除一些朝鲜核武器基础设施为条件，取消部分制裁措施。': 0.1258125677587305, '迄今为止，特朗普寻求与伊朗领导人直接对话的努力都遭到拒绝，但9月的联合国大会提供了另一个与伊朗总统哈桑·鲁哈尼会晤的机会。': 0.12058041710927377, '要做到这一点，就可能需要极大缓解对伊朗的经济施压。': 0.1137452753256049, '特朗普也许认为值得冒这个险，即使这需要他的极端鹰派国家安全顾问约翰·博尔顿下台。': 0.12917175246931265, '文章指出，中美也可能达成部分贸易协议。': 0.13873648863009047, '特朗普将可能利用任何这样的协议“大做文章”，称其为世纪性的协议。': 0.14772785873469854, '曾在奥巴马政府担任高级国家安全官员的朱莉安娜·史密斯说：“特朗普将抢占头条，将自己塑造成领袖。”': 0.1453398265560468, '她说：“未来18个月，我们将看到一系列轻率和鲁莽的举动以及更多的戏剧性事件。': 0.08874793360914503, '所以，坐稳了！”': 0.07405528979642051}\n",
      "\n",
      "\n",
      "参考消息网8月15日报道 英国《卫报》网站8月11日发表题为《为何特朗普的“极限施压”外交政策收效甚微？》的文章，作者为朱利安·博格。文章称，唐纳德·特朗普即将带着没有明显外交胜利的政绩投入2020年大选。当时有人警告他说，这将可能使他失去第二个任期。文章还称，他本能地想要做一笔引人注目的交易。特朗普将可能利用任何这样的协议“大做文章”，称其为世纪性的协议。曾在奥巴马政府担任高级国家安全官员的朱莉安娜·史密斯说：“特朗普将抢占头条，将自己塑造成领袖。”\n"
     ]
    }
   ],
   "source": [
    "print(model.get_summarization_simple_by_sen_embedding(test_3, test_title_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
